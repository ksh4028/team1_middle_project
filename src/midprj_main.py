import os
import pandas as pd
import olefile
import zlib
import unicodedata
import struct
import pickle
from pathlib import Path
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
import os
import shutil
from pathlib import Path
import unicodedata
import sqlite3
import datetime
import sys
from filelock import FileLock
import torch
from dataclasses import dataclass

# LangChain - Vector Stores & Embeddings
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline, HuggingFaceEmbeddings
from langchain_core.messages import HumanMessage

# LangChain - Core
from langchain_core.messages import HumanMessage
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# LangChain - Document Loaders
from langchain_community.document_loaders import PyPDFLoader, UnstructuredPDFLoader

# LangChain - Vector Stores & Embeddings
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline, HuggingFaceEmbeddings
import numpy as np
import faiss
# Transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

# í™˜ê²½ ë³€ìˆ˜
from dotenv import load_dotenv

BASE_DIR =  r"D:\project\TodoPrj" if os.path.exists(r"D:\project\TodoPrj") else "/home/nabi/project"
DATA_DIR =  os.path.join(BASE_DIR, "data")
# .env íŒŒì¼ ë¡œë“œ
ENV_FILE = os.path.join(BASE_DIR, ".env")

CSV_PATH = os.path.join(DATA_DIR,"rfp_files", "data_list.csv")
RFP_DATA_DIR = os.path.join(DATA_DIR, "rfp_files","files")
MY_FILE_VER = f"midprj_01"
#_{os.path.basename(__file__).split('.')[0]}.01"

SQLITEDB_DIR = os.path.join(DATA_DIR, "dbfile")
SQLITEDB_PATH = os.path.join(SQLITEDB_DIR, f"{MY_FILE_VER}.db")

LOG_DIR = os.path.join(DATA_DIR, "log")
LOG_FILE = os.path.join(LOG_DIR, f"{MY_FILE_VER}.txt")
IS_GPU = torch.cuda.is_available()

STORE_VER = "V05"

def init_Env():
    print("í™˜ê²½ ë³€ìˆ˜ ì´ˆê¸°í™” ë° ë””ë ‰í† ë¦¬ ìƒì„±")
    load_dotenv(ENV_FILE)
    if not os.path.exists(SQLITEDB_DIR):
        os.makedirs(SQLITEDB_DIR)
    if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)

init_Env()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â–£ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ 
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## ì‹œê°„ í•¨ìˆ˜
def now_str():
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# ë¬¸ì êµ¬ë¶„ì„  ë° ë©”ì‹œì§€ ì¶œë ¥ í•¨ìˆ˜ ë³µêµ¬
def Lines(text=None, count=100):
    print("â•" * count)
    if text is not None:
        print(text)
        print("â•" * count)


## ë¡œê·¸ í•¨ìˆ˜
def OpLog(log,bLines = False):
    if bLines:
        Lines(log)
    try:
        frame = sys._getframe(1)
        caller_name = frame.f_code.co_name
        caller_line = frame.f_lineno
    except Exception:
        caller_name = "UnknownFunction"
        caller_line = 0

    log_lock_filename = LOG_FILE + ".lock"
    log_content = f"[{now_str()}] {caller_name}:{caller_line}: {log}\n"
    try:
        with FileLock(log_lock_filename, timeout=10):
            with open(LOG_FILE, 'a', encoding='utf-8') as f:
                f.write(log_content)
    except Exception as e:
        print(f"Log write error: {e}")



@dataclass
class PARAMVAR: 
    embedding_model:str = "text-embedding-3-small"
    llm_model:str = "gpt-4o-mini"
    chunk_size:int = 1000
    chunk_overlap:int = 100
    temperature:float = 0.2
    repetition_penalty:float = 1.2
    query:str = ""
    answer:str = ""
    start_time:str = "2000-01-01 00:00:00"
    end_time:str = "2001-01-01 00:00:00"
    is_gpu:bool = IS_GPU
    csv_path:str =  CSV_PATH
    rfp_data_dir:str = RFP_DATA_DIR
    k : int = 5
    is_openai : bool = True
    newCreate : bool = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â–£ SQLite ë°ì´í„°ë² ì´ìŠ¤ í•¸ë“¤ëŸ¬ í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class SQLiteDB:
## SQLiteDB ì´ˆê¸°í™” ë° í…Œì´ë¸” ìƒì„±
    def __init__(self):
        Lines("SQLiteDB ì´ˆê¸°í™” ì‹œì‘")
        self._db_path = SQLITEDB_PATH
        self.connection = None
        self.cursor = None
        self.create_table()
        Lines("SQLiteDB ì´ˆê¸°í™” ì™„ë£Œ")

## ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„±
    def _connect(self):
        self._close()
        self.connection = sqlite3.connect(self._db_path)
        self.cursor = self.connection.cursor()

## ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ
    def _close(self):
        if self.cursor is not None:
            self.cursor.close()
            self.cursor = None
        if self.connection is not None:
            self.connection.close()
            self.connection = None

    ## í•„ìš”í•œ ë°ì´í„° í…Œì´ë¸” ìƒì„± (ì²­í¬ ê¸°ë°˜ ì €ì¥)
    def create_table(self):
        OpLog("í…Œì´ë¸” ìƒì„± ì‹œì‘")
        # ì²­í¬ ë‹¨ìœ„ ì €ì¥ìš© í…Œì´ë¸” (ëª¨ë“  ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ì €ì¥)
        self.execute('''CREATE TABLE IF NOT EXISTS blob_data (
            blob_name TEXT ,
            blob_index INTEGER,
            blob_content BLOB,
            PRIMARY KEY (blob_name, blob_index)
            )
            ''')
        self.execute('''CREATE TABLE IF NOT EXISTS result_data (
            execute_index integer,
            model_item TEXT, -- OpenAI, HuggingFace 
            embedding_model TEXT,
            llm_model TEXT,
            temperature REAL,
            repetition_penalty REAL,          
            query_index INTEGER,
            query TEXT,
            answer TEXT,
            start_time TEXT,
            end_time TEXT,
            PRIMARY KEY (execute_index,model_item,embedding_model,llm_model,temperature,repetition_penalty,query_index)
            )
            ''')
        
        self.execute('''CREATE TABLE IF NOT EXISTS rfp_metadata (
            Notice_no TEXT, 
            Notice_round TEXT,
            project_name TEXT,
            budget REAL,
            agency TEXT,
            publish_date TEXT,
            participation_start_date TEXT,
            participation_end_date TEXT,
            project_summary TEXT,
            file_type TEXT,
            file_name TEXT,
            text_content TEXT,
             PRIMARY KEY (Notice_no)
            )
            ''')
    

    ### ì¼ë°˜ SQL ì¿¼ë¦¬ ì‹¤í–‰
    def execute( self, query):
        self._connect()
        # Lines(f"SQL:{query}") # ë¡œê·¸ ë„ˆë¬´ ë§ìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬
        self.cursor.execute(query)
        self.connection.commit()
        self._close()

    ## SELECT ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼ ë°˜í™˜
    def select(self, sql):
        self._connect()
        cursor = self.connection.execute(sql)
        rows = cursor.fetchall()
        self._close()
        return rows
    
    ## íŒŒë¼ë¯¸í„°í™”ëœ SELECT ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼ ë°˜í™˜ (íŠ¹ìˆ˜ë¬¸ì ì•ˆì „)
    def select_with_params(self, sql, params):
        self._connect()
        self.cursor.execute(sql, params)
        rows = self.cursor.fetchall()
        self._close()
        return rows

    ## ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (í…Œì´ë¸” ì‚­ì œ í›„ ì¬ìƒì„±)
    def clear_db(self):
        OpLog("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹œì‘")
        self.execute('DROP TABLE IF EXISTS blob_data')
        self.execute('DROP TABLE IF EXISTS result_data')
        self.execute('DROP TABLE IF EXISTS rfp_metadata')
        self.create_table()
        OpLog("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    ## BLOB ë°ì´í„° ì²­í¬ ë‹¨ìœ„ ì €ì¥ 
    def save_blob(self, blob_name: str, blob_content: bytes):
        OpLog(f"Blob ì €ì¥ ì‹œì‘: {blob_name} (í¬ê¸°: {len(blob_content) / (1024**3):.2f} GB)")
        
        # 2GB ë‹¨ìœ„ë¡œ ë¶„í•  (2GB = 2 * 1024 * 1024 * 1024 ë°”ì´íŠ¸)
        CHUNK_SIZE = 2 * 1024 * 1024 * 1024  # 2GB
        chunks = []
        for i in range(0, len(blob_content), CHUNK_SIZE):
            chunks.append(blob_content[i:i+CHUNK_SIZE])
        
        OpLog(f"ì´ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨")
        
        self._connect()
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (UPDATE ëŒ€ì‹  DELETE)
        sql_delete = 'DELETE FROM blob_data WHERE blob_name = ?'
        self.cursor.execute(sql_delete, (blob_name,))
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ì €ì¥
        sql_insert = '''
            INSERT INTO blob_data (blob_name, blob_index, blob_content)
            VALUES (?, ?, ?)
        '''
        for index, chunk in enumerate(chunks):
            self.cursor.execute(sql_insert, (blob_name, index, chunk))
            OpLog(f"ì²­í¬ {index}/{len(chunks)-1} ì €ì¥ ì™„ë£Œ (í¬ê¸°: {len(chunk) / (1024**3):.2f} GB)")
        
        self.connection.commit()
        self._close()
        OpLog(f"Blob ì €ì¥ ì™„ë£Œ: {blob_name} ({len(chunks)}ê°œ ì²­í¬)")
    
    ## BLOB ë°ì´í„° ì²­í¬ ë‹¨ìœ„ ë¡œë“œ ë° ë³‘í•©
    def load_blob(self, blob_name: str) -> bytes:
        OpLog(f"Blob ë¡œë“œ ì‹œì‘: {blob_name}")
        self._connect()
        
        # ëª¨ë“  ì²­í¬ë¥¼ blob_index ìˆœì„œëŒ€ë¡œ ë¡œë“œ
        sql = '''
            SELECT blob_index, blob_content FROM blob_data 
            WHERE blob_name = ? 
            ORDER BY blob_index ASC
        '''
        self.cursor.execute(sql, (blob_name,))
        rows = self.cursor.fetchall()
        self._close()
        
        if rows:
            # ì²­í¬ë“¤ì„ ìˆœì„œëŒ€ë¡œ í•©ì¹˜ê¸°
            combined_content = b''
            for index, (blob_index, chunk_content) in enumerate(rows):
                combined_content += chunk_content
                OpLog(f"ì²­í¬ {blob_index} ë¡œë“œ ì™„ë£Œ (ëˆ„ì  í¬ê¸°: {len(combined_content) / (1024**3):.2f} GB)")
            
            OpLog(f"Blob ë¡œë“œ ì™„ë£Œ: {blob_name} ({len(rows)}ê°œ ì²­í¬ ë³‘í•©)")
            return combined_content
        else:
            OpLog(f"Blob ì—†ìŒ: {blob_name}")
            return None
    
    ## ê²°ê³¼ ë°ì´í„° ë¡œë“œ
    def load_results(self, execute_index:int)-> bool:
        sql = '''
            SELECT * FROM result_data 
            WHERE execute_index=?
        '''
        params = (execute_index,)
        rows = self.select_with_params(sql, params)
        if rows:
            return True
        else:
            return False 
    
    ## ê²°ê³¼ ë°ì´í„° ì €ì¥
    def save_results(self, execute_index:int, model_item:str, embedding_model:str, llm_model:str, temperature:float, repetition_penalty:float, query_index:int, query:str, answer:str, start_time:str, end_time:str):
        sql = '''
            INSERT OR REPLACE INTO result_data 
            (execute_index, model_item, embedding_model, llm_model, temperature, repetition_penalty, query_index, query, answer, start_time, end_time)
            VALUES 
            (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        '''
        params = (execute_index, model_item, embedding_model, llm_model, temperature, repetition_penalty, query_index, query, answer, start_time, end_time)
        self._connect()
        self.cursor.execute(sql, params)
        self.connection.commit()
        self._close()
        Lines(f"Result ì €ì¥ ì™„ë£Œ: execute_index={execute_index}, query_index={query_index}, model_item={model_item}, embedding_model={embedding_model}, llm_model={llm_model}\ntemperature={temperature}, repetition_penalty={repetition_penalty}\nonlocal query={query}\nanswer={answer}\nstart_time={start_time}, end_time={end_time}")
        OpLog(f"Result ì €ì¥ ì™„ë£Œ: execute_index={execute_index}, query_index={query_index}")
     
    ## ë©”íƒ€ë°ì´í„° ì €ì¥
    def save_metadata(self, metadata:pd.DataFrame):
        sql = "DELETE FROM rfp_metadata"
        self.execute(sql)
        self._connect()
        sql = '''
            INSERT OR REPLACE INTO rfp_metadata (Notice_no, Notice_round, project_name, budget, agency, publish_date, participation_start_date, participation_end_date, project_summary, file_type, file_name, text_content)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        '''
        # ê³µê³  ë²ˆí˜¸	ê³µê³  ì°¨ìˆ˜	ì‚¬ì—…ëª…	ì‚¬ì—… ê¸ˆì•¡	ë°œì£¼ ê¸°ê´€	ê³µê°œ ì¼ì	ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼	ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼	ì‚¬ì—… ìš”ì•½	íŒŒì¼í˜•ì‹	íŒŒì¼ëª…	í…ìŠ¤íŠ¸

        params_list = [(
            row['ê³µê³  ë²ˆí˜¸'],
            row['ê³µê³  ì°¨ìˆ˜'],
            row['ì‚¬ì—…ëª…'],
            row['ì‚¬ì—… ê¸ˆì•¡'],
            row['ë°œì£¼ ê¸°ê´€'],
            row['ê³µê°œ ì¼ì'],
            row['ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼'],
            row['ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼'],
            row['ì‚¬ì—… ìš”ì•½'],
            row['íŒŒì¼í˜•ì‹'],
            row['íŒŒì¼ëª…'],
            row['í…ìŠ¤íŠ¸']
        ) for _, row in metadata.iterrows()]
        self.cursor.executemany(sql, params_list)
        self.connection.commit()
        self._close()
        OpLog(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(metadata)}ê°œ ë ˆì½”ë“œ")
    
    ## ë©”íƒ€ë°ì´í„° ë¡œë“œ
    def load_metadata(self)-> pd.DataFrame:
        sql = '''
            SELECT * FROM rfp_metadata
        '''
        rows = self.select(sql)
        columns = ['Notice_no', 'Notice_round', 'project_name', 'budget', 'agency', 'publish_date', 'participation_start_date', 'participation_end_date', 'project_summary', 'file_type', 'file_name', 'text_content']
        df = pd.DataFrame(rows, columns=columns)
        OpLog(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")
        return df

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â–£ ë©”íƒ€ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class BidMatePreprocessor:
    ## ìƒì„±ì
    def __init__(self, param:PARAMVAR):
        self.param = param
        self.data_dir = Path(self.param.rfp_data_dir)
        #self.normalize_filenames(self.param.rfp_data_dir)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=param.chunk_size,
            chunk_overlap=param.chunk_overlap
        )
        self.embeddings = None
        if  param.is_openai:
            self.embeddings = OpenAIEmbeddings(
                openai_api_key=os.getenv("OPENAI_API_KEY"),
                model=param.embedding_model
            )
        else:
            self.embeddings = HuggingFaceEmbeddings(
            model_name=param.embedding_model)
            
        self.vector_store = None
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        self.metadata_df = pd.read_csv(self.param.csv_path)
        OpLog(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.metadata_df)}ê°œ ë ˆì½”ë“œ", True)


    ## íŒŒì¼ íƒìƒ‰ê¸°ì—ì„œ í•œê¸€ íŒŒì¼ëª…ì´ ê¹¨ì ¸ ë³´ì´ëŠ”(ììŒ/ëª¨ìŒ ë¶„ë¦¬) í˜„ìƒì€ ì£¼ë¡œ MacOSì—ì„œ ì••ì¶•ëœ íŒŒì¼ì„ Windowsì—ì„œ í’€ì—ˆì„ ë•Œ ë°œìƒí•˜ëŠ”
    ## NFD(Normalization Form Decomposition) ì¸ì½”ë”© ë¬¸ì œ
    ## NFC(Normalization Form Composition)ë¡œ ë³€í™˜í•˜ì—¬ í•´ê²°
    def normalize_filenames(self, directory):
        target_dir = Path(directory)
        if not target_dir.exists():
            print(f"Directory not found: {target_dir}")
            return

        print(f"Scanning directory: {target_dir}")
        count = 0
        
        for file_path in target_dir.iterdir():
            if file_path.is_file():
                original_name = file_path.name
                # Normalize to NFC (NFC is standard for Windows/Linux, NFD is MacOS)
                normalized_name = unicodedata.normalize('NFC', original_name)
                
                if original_name != normalized_name:
                    new_path = target_dir / normalized_name
                    try:
                        # Rename the file
                        file_path.rename(new_path)
                        print(f"Renamed: {original_name} -> {normalized_name}")
                        count += 1
                    except Exception as e:
                        print(f"Error renaming {original_name}: {e}")

        print(f"Normalization complete. {count} files renamed.")


    ## HWP í…ìŠ¤íŠ¸ ì¶”ì¶œ
    def _extract_hwp_text(self, file_path):
        #HWP íŒŒì¼(v5)ì˜ Record êµ¬ì¡°ë¥¼ íŒŒì‹±í•˜ì—¬ í…ìŠ¤íŠ¸(Tag ID 67)ë§Œ ì¶”ì¶œ
        try:
            f = olefile.OleFileIO(file_path)
            dirs = f.listdir()
            text = ""
            for d in dirs:
                if "BodyText" in d:
                    section = f.openstream(d).read()
                    data = None
                    
                    # zlib ë§¤ì§ ë„˜ë²„ ì²´í¬ (0x789c or 0x78da, 0x7801, 0x785e ë“±)
                    if section[:2] in [b'\x78\x9c', b'\x78\xda', b'\x78\x01', b'\x78\x5e']:
                        # âœ“ ì••ì¶•ëœ ë°ì´í„°: zlib ì••ì¶• í•´ì œ
                        decompress_errors = []
                        for wbits in [-15, 15, -zlib.MAX_WBITS, zlib.MAX_WBITS]:
                            try:
                                data = zlib.decompress(section, wbits)
                                OpLog(f"âœ“ HWP BodyText zlib ì••ì¶• í•´ì œ ì„±ê³µ [{os.path.basename(file_path)}]")
                                break
                            except zlib.error as e:
                                decompress_errors.append(f"wbits={wbits}: {str(e)[:30]}")
                                continue
                        
                        if data is None:
                            file_name = os.path.basename(file_path)
                            file_size = len(section)
                            OpLog(f"âš ï¸ HWP ì••ì¶• í•´ì œ ì‹¤íŒ¨ [{file_name}] (í¬ê¸°: {file_size} bytes) - ì²˜ë¦¬ ìŠ¤í‚µ")
                            print(f"âš ï¸ HWP ì••ì¶• í•´ì œ ì‹¤íŒ¨, ê±´ë„ˆëœ€: {file_path}")
                            continue
                    else:
                        # âœ“ ë¹„ì••ì¶• ë°ì´í„°: ì§ì ‘ íŒŒì‹± (HWP 5.0+ì—ì„œ ì¼ë¶€ ìŠ¤íŠ¸ë¦¼ì€ ì••ì¶• ì•ˆ ë¨)
                        file_name = os.path.basename(file_path)
                        magic = section[:2].hex()
                        OpLog(f"âœ“ HWP BodyText ë¹„ì••ì¶• í˜•ì‹ ê°ì§€ [{file_name}] (ë§¤ì§: 0x{magic}) - ì§ì ‘ íŒŒì‹±")
                        data = section
                    
                    pos = 0
                    size_mp = len(data)
                    
                    while pos < size_mp:
                        # 1. Record Header (4 bytes)
                        if pos + 4 > size_mp:
                            break
                        
                        header = struct.unpack('<I', data[pos:pos+4])[0]
                        pos += 4
                        
                        # 2. Tag ID & Length
                        # Tag ID: í•˜ìœ„ 10ë¹„íŠ¸
                        # Length: ìƒìœ„ 12ë¹„íŠ¸
                        tag_id = header & 0x3FF
                        rec_len = (header >> 20) & 0xFFF
                        
                        # ê¸¸ì´ê°€ 0xFFF(4095)ì¸ ê²½ìš° ì¶”ê°€ 4ë°”ì´íŠ¸ì— ì‹¤ì œ ê¸¸ì´ ì €ì¥
                        if rec_len == 0xFFF:
                            if pos + 4 > size_mp:
                                break
                            rec_len = struct.unpack('<I', data[pos:pos+4])[0]
                            pos += 4
                            
                        if pos + rec_len > size_mp:
                            break
                            
                        # 3. í…ìŠ¤íŠ¸ ì¶”ì¶œ (Tag ID 67: HWPTAG_PARA_TEXT ì¶”ì •)
                        # ë””ë²„ê¹… ê²°ê³¼ 67ë²ˆ íƒœê·¸ê°€ ê°€ë³€ ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹´ê³  ìˆìŒ
                        if tag_id == 67: 
                            text_bytes = data[pos:pos+rec_len]
                            try:
                                # UTF-16LE ë””ì½”ë”©
                                decoded = text_bytes.decode('utf-16', errors='ignore')
                                
                                # í…ìŠ¤íŠ¸ ë‚´ë¶€ì˜ ì œì–´ë¬¸ì ë° ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì œê±°
                                # í•œê¸€(ê°€-í£), ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ êµ¬ë‘ì ë§Œ í—ˆìš©
                                import re
                                clean = re.sub(r'[^ê°€-í£a-zA-Z0-9\s.,()\-\[\]]', ' ', decoded)
                                clean = re.sub(r'\s+', ' ', clean).strip()
                                
                                if len(clean) > 0:
                                    text += clean + " "
                            except:
                                pass
                                
                        pos += rec_len
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ê³µë°± ì •ë¦¬
            import re
            return re.sub(r'\s+', ' ', text).strip()
            
        except Exception as e:
            print(f"âŒ HWP ì¶”ì¶œ ì˜¤ë¥˜ ({file_path}): {e}")
            return ""
    ## PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
    def _extract_pdf_text(self,file_path):
        try:
            loader = PyPDFLoader(str(file_path))
            pages = loader.load()
            content = "\n".join([p.page_content for p in pages])
            return content
        except Exception as e:
            print(f"âŒ PDF ì¶”ì¶œ ì˜¤ë¥˜ ({file_path}): {e}")
            return ""
        
    def save_metadata(self):
        db = SQLiteDB() 
        self.metadata_df = pd.read_csv(self.param.csv_path)
        db.save_metadata(self.metadata_df)

    def get_all_docs(self):
        all_docs = []
        for _, row in self.metadata_df.iterrows():
            file_name = unicodedata.normalize('NFC', row['íŒŒì¼ëª…'])
            file_path = os.path.join(self.param.rfp_data_dir, file_name)
            
            if not os.path.exists(file_path):
                print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_name}")
                continue

            # 1. í¬ë§·ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° Document ìƒì„±
            print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {file_name}")
            content = ""
            if file_name.endswith('.pdf'):
                content = self._extract_pdf_text(file_path)
            elif file_name.endswith('.hwp'):
                content = self._extract_hwp_text(file_path)

            # 2. ë©”íƒ€ë°ì´í„° ê²°í•©
            doc = Document(
                page_content=content,
                metadata={
                    "source": file_name,
                    "Notice_no": row['ê³µê³  ë²ˆí˜¸'],
                    "Notice_round": row['ê³µê³  ì°¨ìˆ˜'],
                    "project_name": row['ì‚¬ì—…ëª…'],
                    "budget": row['ì‚¬ì—… ê¸ˆì•¡'],
                    "agency": row['ë°œì£¼ ê¸°ê´€'],
                    ## ê³µê°œì¼ì
                    "publish_date": row['ê³µê°œ ì¼ì'],
                    ## ì…ì°° ì°¸ì—¬ ì—¬ë¶€
                    "participation_start_date": row['ì…ì°° ì°¸ì—¬ ì‹œì‘ì¼'],
                    ## ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼
                    "participation_end_date": row['ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼'],
                    ## ì‚¬ì—…ìš”ì•½
                    "project_summary": row['ì‚¬ì—… ìš”ì•½'],
                    ## íŒŒì¼í˜•ì‹
                    "file_type": row['íŒŒì¼í˜•ì‹'],
                    ## íŒŒì¼ëª…
                    "file_name": row['íŒŒì¼ëª…'],
                    
                }
            )
            Lines(doc.metadata)
            
            # 3. ì²­í‚¹ ì ìš©
            splits = self.text_splitter.split_documents([doc])
            all_docs.extend(splits)
        return all_docs



    ## FAISS ì´ë¦„ ìƒì„±
    def make_faiss_name(self):
        vector_name = f"{self.param.embedding_model.replace('/', '_')}_{self.param.llm_model.replace('/', '')}"
        faiss_name = f"faiss_store_{vector_name}_cs_{self.param.chunk_size}_co_{self.param.chunk_overlap}_{STORE_VER}"
        return faiss_name

    ## ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ í™•ì¸ ë° ë¡œë“œ
    def _check_vector_store_exists(self, newCreate):
        vector_store = None
        faiss_name = self.make_faiss_name()
        if not newCreate:
            db = SQLiteDB()
            blob_bytes = db.load_blob(faiss_name)
            if blob_bytes:
                try:
                    vector_store = FAISS.deserialize_from_bytes(
                        blob_bytes,
                        self.embeddings,
                        allow_dangerous_deserialization=True,
                    )
                    print(f"âœ… ê¸°ì¡´ Vector DB ë¡œë“œ ì™„ë£Œ: {faiss_name}")
                except Exception as e:
                    print(f"âš ï¸ Vector DB ì—­ì§ë ¬í™” ì‹¤íŒ¨: {e}")
            else:
                print(f"âš ï¸ ê¸°ì¡´ Vector DB ì—†ìŒ: {faiss_name}")
        return vector_store

    def get_hugging_vector_store(self,newCreate:bool = False):
        db = SQLiteDB() 
        vector_store = self._check_vector_store_exists(newCreate)
        if not vector_store is None:
            return vector_store
        faiss_name = self.make_faiss_name()
        if not newCreate:
            blob_bytes = db.load_blob(faiss_name)
            if blob_bytes:
                try:
                    self.vector_store = FAISS.deserialize_from_bytes(
                        blob_bytes,
                        self.embeddings,
                        allow_dangerous_deserialization=True,
                    )
                    print(f"âœ… ê¸°ì¡´ Vector DB ë¡œë“œ ì™„ë£Œ: {faiss_name}")
                    return self.vector_store
                except Exception as e:
                    print(f"âš ï¸ Vector DB ì—­ì§ë ¬í™” ì‹¤íŒ¨: {e}")
            else:
                print(f"âš ï¸ ê¸°ì¡´ Vector DB ì—†ìŒ: {faiss_name}")
        else :
            print(f"âš ï¸ ìƒˆë¡œ ìƒì„±: {faiss_name}")
                  
        all_docs = self.get_all_docs()
        embedding_dim = len(self.embeddings.embed_query("hello world"))
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„: ë²¡í„° ì •ê·œí™” + IndexFlatIP
        index = faiss.IndexFlatIP(embedding_dim)
        texts = [doc.page_content for doc in all_docs]
        vectors = [self.embeddings.embed_query(text) for text in texts]
        vectors = np.array(vectors, dtype=np.float32)
        if vectors.ndim == 1:
            vectors = vectors.reshape(1, -1)
        # L2 ì •ê·œí™”
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        vectors = vectors / (norms + 1e-10)
        docstore_dict = {str(i): doc for i, doc in enumerate(all_docs)}
        index_to_docstore_id = {i: str(i) for i in range(len(all_docs))}
        self._vector_store = FAISS(
            embedding_function=self.embeddings,
            index=index,
            docstore=InMemoryDocstore(docstore_dict),
            index_to_docstore_id=index_to_docstore_id,
        )
        self._vector_store.index.add(vectors)
         # 4. Vector DB ìƒì„± ë° ì €ì¥ (Scenario B: OpenAI ê¸°ë°˜)
        print(f"ğŸš€ ì´ {len(all_docs)}ê°œ ì²­í¬ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤...")
        self.vector_store = FAISS.from_documents(all_docs, self.embeddings)
        blob_bytes = self.vector_store.serialize_to_bytes()
        db = SQLiteDB() 
        db.save_blob(faiss_name, blob_bytes)
        return self.vector_store


  
    def get_openai_vector_store(self,newCreate):
        faiss_name = self.make_faiss_name()
        vector_store = self._check_vector_store_exists(newCreate)
        if not vector_store is None:
            return vector_store
        db = SQLiteDB() 
        all_docs = self.get_all_docs()
        
        # 4. Vector DB ìƒì„± ë° ì €ì¥ (Scenario B: OpenAI ê¸°ë°˜)
        print(f"ğŸš€ ì´ {len(all_docs)}ê°œ ì²­í¬ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤...")
        self.vector_store = FAISS.from_documents(all_docs, self.embeddings)
        self.vector_store.save_local(faiss_name)
        blob_bytes = self.vector_store.serialize_to_bytes()
        db = SQLiteDB() 
        db.save_blob(faiss_name, blob_bytes)
        return self.vector_store
   

    def get_vector_store(self,newCreatl):
        if self.param.is_openai:
            return self.get_openai_vector_store(newCreatl)
        else:
            return self.get_openai_vector_store(newCreatl)
            

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â–£ ë² ì´ìŠ¤ ëª¨ë¸ í´ë˜ìŠ¤ 
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class BaseModel():
    def __init__(self,param:PARAMVAR):
        self._my_name = f"embedding:{param.embedding_model}_llm:{param.llm_model}"
        self._param = param
        self._vector_store = None
        self._llm = None

    def rag_search(self, question):
        pass

    def query_answer(self,index,query):
        pass
    
    def clear_mem(self):
        import gc
        if self._vector_store is not None:
            del self._vector_store
            self._vector_store = None
        if self._faiss_db is not None:
            del self._faiss_db
            self._faiss_db = None
        torch.cuda.empty_cache()
        gc.collect()

      
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â–£ OpenAI ë° HuggingFace ëª¨ë¸ í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class OpenAIModel(BaseModel):
    def __init__(self,param:PARAMVAR):
        super().__init__(param)
        Lines(f"Make Model :: My_name:{self._my_name}, embedding_model:{self._param.embedding_model},llm_model:{self._param.llm_model},chunk_size:{self._param.chunk_size},chunk_overlap:{self._param.chunk_overlap},temperature:{self._param.temperature},repetition_penalty:{self._param.repetition_penalty}")
        processor = BidMatePreprocessor(self._param)
        self._vector_store = processor.get_vector_store(self._param.newCreate)

    def make_model(self):
        OpLog(f"OpenAI LLM ìƒì„± ì‹œì‘: {self._param.llm_model}")
        self._llm = ChatOpenAI(model=self._param.llm_model)
        OpLog(f"OpenAI LLM ìƒì„± ì™„ë£Œ")
        
        
    def rag_search(self, question):
        # ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰
        results = self._vector_store.similarity_search(question, k=self._param.k)
        # ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        context = "\n---\n".join([r.page_content for r in results])
        # LLMì—ê²Œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì„ í•¨ê»˜ ì „ë‹¬í•˜ì—¬ ì‘ë‹µ ìƒì„±
        prompt = f"""ë‹¤ìŒ ë©”íƒ€ë°ì´í„°ì™€ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

Context: 
{context}

Question: {question}

Answer:"""
        response = self._llm.invoke([HumanMessage(content=prompt)])
        return response.content



class HugginFaceModel(BaseModel):
    def __init__(self,param:PARAMVAR):
        super().__init__(param)
        param.is_openai = False
        Lines(f"Make Model :: My_name:{self._my_name}, embedding_model:{self._param.embedding_model},llm_model:{self._param.llm_model},chunk_size:{self._param.chunk_size},chunk_overlap:{self._param.chunk_overlap},temperature:{self._param.temperature},repetition_penalty:{self._param.repetition_penalty}")
        processor = BidMatePreprocessor(param)
        self._vector_store = processor.get_vector_store(self._param.newCreate)
    
    def make_model(self):
        Lines(f"Make Model :: embedding_model_name:{self._param.embedding_model},llm_model:{self._param.llm_model},model_name:{self._my_name},temperature:{self._param.temperature},repetition_penalty:{self._param.repetition_penalty}")
        from transformers import AutoModelForCausalLM, AutoTokenizer        
        retriever = self._vector_store.as_retriever()
        if self._param.is_gpu:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                llm_int8_enable_fp32_cpu_offload=True,
            )
        else:
            bnb_config = None
        device_map = "auto"
        if( self._param.is_gpu):
            device_map = "auto"
        else:
            device_map = None

        Lines("Make AutoModelForCausalLM")
        OpLog(f"AutoModelForCausalLM ë¡œë“œ ì‹œì‘: {self._param.llm_model}")
        model = AutoModelForCausalLM.from_pretrained(
            self._param.llm_model,
            quantization_config= bnb_config if self._param.is_gpu else None,
            device_map=device_map,
            trust_remote_code=True,
        )
        OpLog(f"AutoModelForCausalLM ë¡œë“œ ì™„ë£Œ")
        OpLog(f"Tokenizer ë¡œë“œ ì‹œì‘: {self._param.llm_model}")
        tokenizer = AutoTokenizer.from_pretrained(self._param.llm_model)
        OpLog(f"Tokenizer ë¡œë“œ ì™„ë£Œ")
        from transformers import pipeline
        Lines("Make LLM pipeline")
        OpLog(f"LLM Pipeline ìƒì„± ì‹œì‘")
        llm_pipeline = pipeline(
            model=model,
            tokenizer=tokenizer,
            task="text-generation",
            do_sample=True,
            temperature=self._param.temperature,
            repetition_penalty=self._param.repetition_penalty,
            return_full_text=False,
            max_new_tokens=1000,
        )
        llm = HuggingFacePipeline(pipeline=llm_pipeline)
        chat_model = ChatHuggingFace(llm=llm)
        template = """ë‹¤ìŒ ë©”íƒ€ë°ì´í„°ì™€ ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
        Context:
        {context}
        Question:
        {question}
        """
        prompt = PromptTemplate.from_template(template)

        from langchain_core.runnables import RunnablePassthrough
        from langchain_core.output_parsers import StrOutputParser

        def format_docs(docs):
            print(docs)
            return "\n\n".join(doc.page_content for doc in docs)

        self._retrieval_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        OpLog(f"Retrieval Chain ìƒì„± ì™„ë£Œ")
    
    def rag_search(self, question):
        # LLMì—ì„œ ìµœì¢… ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
        answer = self._retrieval_chain.invoke(question)
        return answer

def Execute_Model(is_openai: bool, chunk_size: int, chunk_overlap: int, temperature: float, repetition_penalty: float, newCreate: bool, k: int, 
                  embedding_model: str , llm_model: str):
    param = PARAMVAR()
    param.is_openai = is_openai
    param.chunk_size =  chunk_size
    param.chunk_overlap = chunk_overlap
    param.temperature = temperature
    param.repetition_penalty = repetition_penalty
    param.newCreate = newCreate
    param.k = k
    model = None
    if( param.is_openai):
        param.embedding_model = embedding_model
        param.llm_model = llm_model
        model = OpenAIModel(param)
    else:
        param.embedding_model = embedding_model
        param.llm_model = llm_model
        model = HugginFaceModel(param)
    model.make_model()
    ## í…ŒìŠ¤íŠ¸ ì§ˆì˜ë¥¼ ì…ë ¥ ë°›ëŠ”ë‹¤. 
    ## ctrl+c ë¡œ ì¢…ë£Œ
    while True: 
        query = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+C): ")
        answer = model.rag_search(query)
        print("ë‹µë³€:")
        Lines(answer)

def Execute_ModelEx(is_openai: bool, chunk_size: int, chunk_overlap: int, temperature: float, repetition_penalty: float, newCreate: bool, k: int, 
                  embedding_model: str , llm_model: str):
    param = PARAMVAR()
    param.is_openai = is_openai
    param.chunk_size =  chunk_size
    param.chunk_overlap = chunk_overlap
    param.temperature = temperature
    param.repetition_penalty = repetition_penalty
    param.newCreate = newCreate
    param.k = k
    model = None
    if( param.is_openai):
        param.embedding_model = embedding_model
        param.llm_model = llm_model
        model = OpenAIModel(param)
    else:
        param.embedding_model = embedding_model
        param.llm_model = llm_model
        model = HugginFaceModel(param)
    model.make_model()
    ## í…ŒìŠ¤íŠ¸ ì§ˆì˜ë¥¼ ì…ë ¥ ë°›ëŠ”ë‹¤. 
    ## ctrl+c ë¡œ ì¢…ë£Œ
    while True: 
        query = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+C): ")
        answer = model.rag_search(query)
        print("ë‹µë³€:")
        Lines(answer)


#text-embedding-3-small" gpt-5-mini"
#nlpai-lab/KoE5 --llm_model nlpai-lab/KULLM3

if __name__ == "__main__":
    param = PARAMVAR()
    param.is_openai = False 
    param.chunk_size = 1000
    param.chunk_overlap = 100
    param.temperature = 0.7
    param.repetition_penalty = 1.2
    param.newCreate = False 
    param.k = 5
    model = None
    if( param.is_openai):
        param.embedding_model = "text-embedding-3-small"
        param.llm_model = "gpt-5-mini"
        model = OpenAIModel(param)
    else:
        param.embedding_model ="BAAI/bge-m3" # "nlpai-lab/KoE5"
        param.llm_model = "nlpai-lab/KULLM3"
        model = HugginFaceModel(param)
    model.make_model()
    ## í…ŒìŠ¤íŠ¸ ì§ˆì˜ë¥¼ ì…ë ¥ ë°›ëŠ”ë‹¤. 
    ## ctrl+c ë¡œ ì¢…ë£Œ
    while True: 
        query = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+C): ")
        answer = model.rag_search(query)
        print("ë‹µë³€:")
        Lines(answer)
